import pandas as pd
from src.LLM_answers import GEMINI_KEY_PATH
import google.generativeai as genai
import json

with open(GEMINI_KEY_PATH) as f:
    GEMINI_API_KEY = f.read().strip()
genai.configure(api_key=GEMINI_API_KEY)

def evaluate_landmark_name(predicted_answer, true_answer):    
    model = genai.GenerativeModel(model_name="gemini-1.5-flash")
    
    # Construct the prompt to ask the LLM to evaluate the match between the two landmarks
    prompt = (
        f"I have two landmark names. The first is the predicted landmark: '{predicted_answer}'. "
        f"The second is the true landmark: '{true_answer}'.\n\n"
        "Please evaluate whether the predicted landmark matches the true landmark. "
        "Respond with either 'True' or 'False'."
    )
    
    # Generate the evaluation using the LLM)
    llm_response = model.generate_content(prompt)
    evaluation = llm_response.text.replace("\n", "").strip()
    
    # Ensure the response is either "Correct" or "Incorrect"
    if "True" in evaluation:
        return "True"
    else:
        return "False"

def evaluate_retrieved_images(retrieved_names, landmarks_list):
    evaluation = []
    for retrieved_name, true_name in zip(retrieved_names, landmarks_list):
        correct = evaluate_landmark_name(retrieved_name, true_name)
        evaluation.append({"retrieved_name": retrieved_name, "true_name": true_name, "correct": correct})
    accuracy = sum(evaluation[i]["correct"] == "True" for i in range(len(evaluation))) / len(evaluation)
    return accuracy, evaluation

def evaluate_generated_images(generated_imgs, landmarks_list):
    model = genai.GenerativeModel(model_name="gemini-1.5-flash")
    
    evaluations = []
    for landmark, img in zip(landmarks_list, generated_imgs):
        answer = model.generate_content([
            (
                f"Is this img from {landmark}? \n"
                f"- Answer in JSON format: "
                f'{{"question_landmark": <landmark in question>, "img_landmark": <img_landmark>, "Explanation": "<provide here explanation>", "bool_answer": "<yes/no>"}}\n'
                f"- Ensure the output is properly formatted as valid JSON, with no extraneous characters, code blocks, or newline escape sequences.\n"
                f"- Do not include any surrounding backticks or labels like 'json'. The output should be a clean, parsable JSON object.\n\n"
            ),
            img
        ]).text
        evaluations.append(answer)
    
    # Extract answers from the evaluations
    bool_answers = []
    question_landmarks = []
    img_landmarks = []
    explanations = []
    for evaluation in evaluations:
        try:
            # Parse the JSON string into a Python dictionary
            eval_dict = json.loads(evaluation)
            # Extract the 'bool_answer' value and add to the list
            bool_answers.append(eval_dict['bool_answer'].lower())
            question_landmarks.append(eval_dict['question_landmark'])
            img_landmarks.append(eval_dict['img_landmark'])
            explanations.append(eval_dict['Explanation'])
        except json.JSONDecodeError:
            print(f"Failed to parse evaluation: {evaluation}")
    evaluation = {
        'bool_answers': bool_answers,
        'question_landmarks': question_landmarks,
        'img_landmarks': img_landmarks,
        'explanations': explanations
    }
    
    # Calculate accuracy
    accuracy = sum(answer.lower() == 'yes' for answer in bool_answers) / len(landmarks_list)
    return accuracy, evaluation

def compare_results_Use_Case_1(RAG_results, baseline_results):
    # TODO: this is a draft code generated by the AI, please complete it
    # Create a comparison dictionary
    comparison = {
        "id": [],
        "RAG_time": [],
        "Baseline_time": [],
        "Time_difference": [],
        "RAG_accuracy": [],
        "Baseline_accuracy": [],
        "Accuracy_difference": []
    }

    # Iterate over results to compute time and accuracy differences
    for rag, baseline in zip(RAG_results, baseline_results):
        comparison["id"].append(rag["id"])
        rag_time = pd.to_datetime(rag["end_time"]) - pd.to_datetime(rag["start_time"])
        baseline_time = pd.to_datetime(baseline["end_time"]) - pd.to_datetime(baseline["start_time"])
        comparison["RAG_time"].append(rag_time.total_seconds())
        comparison["Baseline_time"].append(baseline_time.total_seconds())
        comparison["Time_difference"].append((baseline_time - rag_time).total_seconds())

        comparison["RAG_accuracy"].append(rag["accuracy"])
        comparison["Baseline_accuracy"].append(baseline["accuracy"])
        comparison["Accuracy_difference"].append(rag["accuracy"] - baseline["accuracy"])

    # Convert to DataFrame for better visualization
    comparison_df = pd.DataFrame(comparison)
    return comparison_df

def compare_results_Use_Case_2(RAG_results, baseline_results):
    # TODO: complete
    raise NotImplementedError

def save_results(RAG_results, baseline_results):
    # TODO: this is a draft code generated by the AI, please complete it
    # Convert results to a dictionary format
    combined_results = {
        "id": [],
        "travel_plan": [],
        "landmarks_list": [],
        "images": [],
        "accuracy": [],
        "start_time": [],
        "end_time": [],
        "response_by": [],
        "use_case": []
    }

    # Append RAG results
    for result in RAG_results:
        for key in combined_results:
            combined_results[key].append(result[key])
    
    # Append baseline results
    for result in baseline_results:
        for key in combined_results:
            combined_results[key].append(result[key])

    # Convert dictionary to DataFrame
    results_df = pd.DataFrame(combined_results)
    save_results_to_file(results_df)
    return results_df

def save_results_to_file(results_df, filename='results.json'):
    # Convert DataFrame to JSON
    results_json = results_df.to_json(orient='records')
    # Save to a file
    with open(filename, 'w') as file:
        file.write(results_json)
    print(f"Results saved to {filename}")